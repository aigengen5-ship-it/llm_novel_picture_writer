import subprocess
import time
import requests

# ì„¤ì •
CONTAINER_NAME = "frosty_engelbart"
PORT = 8081

def check_container_port_free():
    """ë„ì»¤ ë‚´ë¶€ì—ì„œ 8081 í¬íŠ¸ê°€ ì§„ì§œë¡œ ë¹„ì—ˆëŠ”ì§€ í™•ì¸"""
    # netstatì´ ë„ì»¤ì— ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, lsofë‚˜ /proc/net/tcpë¥¼ í™•ì¸í•˜ëŠ” ê²Œ ì •í™•í•˜ì§€ë§Œ
    # ê°„ë‹¨í•˜ê²Œ 'pkill -0'ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ë°©ì‹ì„ ì¶”ì²œ

    # "llama-server"ë¼ëŠ” ì´ë¦„ì˜ í”„ë¡œì„¸ìŠ¤ê°€ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸ (exit code 0ì´ë©´ ì‚´ì•„ìˆìŒ)
    result = subprocess.run(
        f"docker exec {CONTAINER_NAME} pkill -0 -f llama-server",
        shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
    )
    # í”„ë¡œì„¸ìŠ¤ê°€ ì—†ìœ¼ë©´(1) í¬íŠ¸ë„ ë¹ˆ ê²ƒìœ¼ë¡œ ê°„ì£¼
    return result.returncode != 0

def switch_model(run_model, gpu_layers=99, context_size=32768):
    print(f"ğŸ”„ Switching to model: {run_model}...")

    if run_model == "gemma-3-27b" or run_model.find("gemma") > -1:
        run_script = "./llama.cpp/build/bin/llama-server -m /models/mlabonne/gemma-3-27b-it/gemma-3-27b-it-abliterated.q8_0.gguf -c 65536 -ngl 999 -fa on --host 0.0.0.0 --port 8081 --temp 0.9 --top-p 0.9 --repeat-penalty 1.1 -ts 1,1  --no-mmap"
    elif run_model == "glm-4.6v":
        run_script = "./llama.cpp/build/bin/llama-server -m /models/huihui-ai/Huihui-GLM-4.6V-abliterated-GGUF/Huihui-GLM-4.6V-abliterated-Q4_K_M-00001-of-00008.gguf -c 32768 -ngl 37 -fa on --host 0.0.0.0 --port 8081 --temp 0.8 --top-p 0.9 --repeat-penalty 1.1 -n -1 --no-mmap"
    elif run_model == "glm-4.5a":
        run_script = "./llama.cpp/build/bin/llama-server -m /models/bartowski/ArliAI_GLM-4.5-Air-Derestricted-GGUF/ArliAI_GLM-4.5-Air-Derestricted-Q5_K_M-00001-of-00003.gguf  -c 32768 -ngl 33 -fa on --host 0.0.0.0 --port 8081 --temp 0.8 --top-p 0.9 --repeat-penalty 1.1 -n -1 --no-mmap  -ctk f16 -ctv f16"
    elif run_model == "gpt-oss-120b":
        run_script = "./llama.cpp/build/bin/llama-server -m /models/bartowski/kldzj_gpt-oss-120b-heretic-GGUF/kldzj_gpt-oss-120b-heretic-bf16-00001-of-00002.gguf -c 32768 -ngl 32 -fa on --host 0.0.0.0 --port 8081 -ts 1,1 --no-mmap"        
    elif run_model == "hyperclovax":
        run_script = "./llama.cpp/build/bin/llama-server -m /models/naver/hyperclovax/HyperCLOVAX-SEED-Think-32B-heretic2.Q8_0.gguf -c 65536 -ngl 999 -fa on -ctk q8_0 -ctv q8_0 --host 0.0.0.0 --port 8081 --temp 1.0 --top-p 0.9 --repeat-penalty 1.1 -n -1 --no-mmap"
    elif run_model == "midnight-miqu":
        run_script = "./llama.cpp/build/bin/llama-server -m /models/Midnight-Miqu/MQ/temp.gguf -c 25565 -ngl 999 -fa on -ctk q8_0 -ctv q8_0 --host 0.0.0.0 --port 8081 --temp 1.0 --top-p 0.9 --repeat-penalty 1.1 -n -1"
    else:
        print("âœ… No model is founded")
        exit()

    # 1. ê¸°ì¡´ llama-server í”„ë¡œì„¸ìŠ¤ ì£½ì´ê¸° (ë„ì»¤ ë‚´ë¶€)
    kill_cmd = f"docker exec {CONTAINER_NAME} pkill llama-server -9"
    subprocess.run(kill_cmd, shell=True)
    time.sleep(30) # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ëŒ€ê¸°
    print("âœ… Killing process is done and waiting port is ready.")

    for _ in range(10):
        if check_container_port_free():
            print("âœ… Port 8081 is free.")
            break
        time.sleep(1)
        print(".", end="", flush=True)
    else:
        print("\nâŒ Error: Old process is stuck. Cannot switch model.")
        return False    

    # 2. ìƒˆ ëª¨ë¸ë¡œ ì„œë²„ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
    # nohupì„ ì¨ì•¼ ë„ì»¤ execê°€ ì•ˆ ë©ˆì¶”ê³  ë°”ë¡œ ëŒì•„ì˜´
    start_cmd = (
        f"docker exec {CONTAINER_NAME} /bin/bash -c 'export HSA_OVERRIDE_GFX_VERSION=9.0.6 && export HIP_VISIBLE_DEVICES=0,1 && nohup {run_script} > /llama_log.txt 2>&1 &'"
    )

    subprocess.run(start_cmd, shell=True)

    print("â³ Waiting for server to start...")
    for i in range(600): # ìµœëŒ€ 60ì´ˆ
        # (A) ì„œë²„ ì‘ë‹µ í™•ì¸
        try:
            res = requests.get(f"http://localhost:{PORT}/health", timeout=1)
            if res.status_code == 200:
                print("\nâœ… New model loaded successfully!")
                return True
        except:
            pass
            
        # (B) â˜…í•µì‹¬â˜… ì„œë²„ í”„ë¡œì„¸ìŠ¤ê°€ ì£½ì—ˆëŠ”ì§€ í™•ì¸ (Early Exit)
        if check_container_port_free(): 
            # í¬íŠ¸ê°€ ë¹„ì–´ìˆë‹¤ëŠ” ê±´ = ë°©ê¸ˆ í‚¨ ì„œë²„ê°€ ì£½ì—ˆë‹¤ëŠ” ëœ»
            print(f"\nâŒ Server crashed immediately! Check logs.")
            # ë¡œê·¸ ì¶œë ¥í•´ì„œ ì›ì¸ ë³´ì—¬ì£¼ê¸°
            subprocess.run(f"docker exec {CONTAINER_NAME} tail -n 5 /llama_log.txt", shell=True)
            return False
            
        time.sleep(1)
        print(".", end="", flush=True)

    print("\nâŒ Timeout.")
    return False

#switch_model("gemma-3-27b")
